# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PW7x__iDhLj1EyMYWAaJZRBGHMktupP7
"""

# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
np.random.seed(42)
num_samples = 1000

# Assuming three features for simplicity
square_footage = np.random.randint(1000, 5000, num_samples)
num_bedrooms = np.random.randint(1, 6, num_samples)
location = np.random.choice(['Suburb', 'Urban', 'Rural'], num_samples)
price = 100000 + 75 * square_footage + 5000 * num_bedrooms + (location == 'Urban') * 20000 + np.random.normal(0, 10000, num_samples)

# Create DataFrame
data = pd.DataFrame({'SquareFootage': square_footage, 'NumBedrooms': num_bedrooms, 'Location': location, 'Price': price})

# Data preprocessing and feature engineering steps...
# Convert 'Location' to one-hot encoding
data = pd.get_dummies(data, columns=['Location'], drop_first=True)

# Split the data
features = data.drop('Price', axis=1)
target = data['Price']
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Model selection and training
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Model evaluation
predictions = model.predict(X_test)
mae = mean_absolute_error(y_test, predictions)
print(f'Mean Absolute Error: {mae}')

# Visualization
plt.scatter(y_test, predictions)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted House Prices')
plt.show()